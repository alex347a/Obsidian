![[Pasted image 20251009190624.png]]
### Pipelining

| Term                                 | Definition                                                                                                                                                                                                                                                                 |
| ------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Pipelining**                       | A processing technique where the execution of an instruction is divided into multiple discrete stages. Each stage is handled by dedicated hardware, allowing multiple instructions to be processed simultaneously at different stages.                                     |
| **Instruction Fetch (Stage 1)**      | The pipeline stage that retrieves the next instruction from memory.                                                                                                                                                                                                        |
| **Instruction Decode (Stage 2)**     | The pipeline stage that deciphers the instruction to determine its type and identify the operands it requires.                                                                                                                                                             |
| **Operand Fetch (Stage 3)**          | The pipeline stage that locates and retrieves the required operands, either from registers or from memory.                                                                                                                                                                 |
| **Instruction Execution (Stage 4)**  | The pipeline stage that performs the actual operation of the instruction, typically by processing the operands through the CPU's data path (e.g., the ALU).                                                                                                                |
| **Write Back (Stage 5)**             | The pipeline stage that writes the result of the instruction execution back to the appropriate register.                                                                                                                                                                   |
| **Pipeline Stage / Unit**            | A dedicated section of hardware within the pipeline responsible for one specific phase of instruction processing.                                                                                                                                                          |
| **Latency**                          | The total time required to complete a single instruction from start to finish. In an n-stage pipeline with a cycle time T, the latency is **n * T**.                                                                                                                       |
| **Processor Bandwidth (Throughput)** | The number of instructions the CPU can complete per second. It is a measure of the pipeline's processing rate.                                                                                                                                                             |
| **Throughput Calculation**           | With a cycle time of T nsec, the processor can complete **1/T** billion instructions per second. This translates to **1000/T MIPS** (Millions of Instructions Per Second).                                                                                                 |
| **Prefetching**                      | An early technique to speed up execution by fetching instructions from memory in advance and storing them in a buffer (prefetch buffer) so they are ready when needed. Pipelining is a more advanced form of this concept.                                                 |
| **Prefetch Buffer**                  | A special set of registers that holds instructions fetched from memory in advance of their execution.                                                                                                                                                                      |
| **Trade-off**                        | Pipelining creates a trade-off: it increases **processor bandwidth (throughput)** by allowing a new instruction to complete on every clock cycle, but it does not reduce the **latency** of a single instruction (it may even increase it slightly due to stage overhead). |
| **Analogy**                          | Like an assembly line in a cake factory, where different workers (stages) perform specific tasks (placing box, adding cake, sealing, labeling) on different cakes (instructions) simultaneously.                                                                           |
### Cache

| Term                                   | Definition                                                                                                                                                                                         |
| -------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Cache Memory**                       | A small, very fast piece of memory located between the CPU and main memory. It stores the most heavily used memory words to dramatically reduce average memory access time.                        |
| **Memory-CPU Speed Gap**               | The fundamental problem where CPUs are significantly faster than main memory (DRAM). After issuing a memory request, the CPU must wait many cycles for the data to arrive, stalling execution.     |
| **Stalling**                           | A simple solution to the memory gap: the CPU halts (stalls) execution when an instruction tries to use a memory word that has not yet arrived from slow memory. This severely impacts performance. |
| **Locality Principle**                 | The observation that memory references in any short time interval tend to cluster in a few areas of memory. This principle is the foundation that makes caching effective.                         |
| **Temporal Locality**                  | The tendency for a memory location, once accessed, to be accessed again soon. (e.g., variables in a loop).                                                                                         |
| **Spatial Locality**                   | The tendency for a memory location, once accessed, to have its nearby locations accessed soon. (e.g., sequential instruction execution, array traversal).                                          |
| **Hit Ratio (h)**                      | The fraction of all memory references that are satisfied by the cache. A high hit ratio is crucial for good performance.                                                                           |
| **Miss Ratio**                         | The fraction of memory references that are _not_ in the cache. Calculated as `1 - h`.                                                                                                              |
| **Cache Hit**                          | When the CPU finds the data it needs in the cache. This results in a fast access.                                                                                                                  |
| **Cache Miss**                         | When the data the CPU needs is _not_ in the cache. This forces a slow access to main memory.                                                                                                       |
| **Mean Access Time**                   | The average time to access memory. Formula: **`c + (1 - h)m`**, where `c` is cache access time, `m` is main memory access time, and `h` is the hit ratio.                                          |
| **Cache Line (Block)**                 | The fixed-size unit of data that is transferred between main memory and the cache. On a miss, the entire line containing the requested word is fetched.                                            |
| **Line Size (Block Size)**             | The number of bytes in a cache line. Larger lines exploit spatial locality more effectively.                                                                                                       |
| **Unified Cache**                      | A single cache that holds both instructions and data. Simpler design but cannot service an instruction fetch and a data fetch simultaneously.                                                      |
| **Split Cache (Harvard Architecture)** | Separate caches for instructions (I-cache) and data (D-cache). Allows a pipelined processor to fetch an instruction and a data operand in parallel.                                                |
| **Primary Cache (L1)**                 | The smallest and fastest cache, typically located directly on the CPU chip.                                                                                                                        |
| **Secondary Cache (L2)**               | A larger, slower cache that sits between the L1 cache and main memory. It may be on-chip or in the same package as the CPU.                                                                        |
| **Tertiary Cache (L3)**                | An even larger, slower cache that is shared between multiple CPU cores, sitting further from the core than L2.                                                                                     |
| **Cache Design Trade-offs**            | The balancing act between cache **size** (bigger is better for hit ratio but slower to access), **line size**, **organization**, and **number of levels**.                                         |
![[Pasted image 20251009194906.png]]
### Microarchitecture ways of optimization

| Term                                    | Definition                                                                                                                                                                                                                                                      |
| --------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Microarchitecture Level**             | The level of computer organization that implements the Instruction Set Architecture (ISA) using a specific data path and control unit.                                                                                                                          |
| **Design Trade-off**                    | The fundamental compromise in computer design, most notably between **speed** and **cost** (complexity, chip area/"real estate").                                                                                                                               |
| **Path Length**                         | The number of clock cycles needed to execute a set of instructions (or a single instruction). A primary target for optimization.                                                                                                                                |
| **Clock Cycle**                         | The fundamental unit of time for a synchronous CPU. The CPU performs one "step" per cycle.                                                                                                                                                                      |
| **Cycle Time**                          | The duration of one clock cycle. A shorter cycle time allows a faster clock speed.                                                                                                                                                                              |
| **Speed vs. Cost Trade-off**            | The central design conflict: faster components and more parallel hardware (higher cost, more real estate) versus simpler, slower components (lower cost, less real estate).                                                                                     |
| **Three Approaches to Increase Speed**  | 1. **Reduce Path Length:** Fewer cycles per instruction.  <br>2. **Shorter Clock Cycle:** Simplify organization to reduce cycle time.  <br>3. **Overlap Execution:** Perform stages of different instructions simultaneously (pipelining).                      |
| **Real Estate**                         | A metaphor for the physical silicon area required for a circuit on a chip. Larger, more complex chips are more expensive to manufacture.                                                                                                                        |
| **Decoder**                             | A circuit that takes a compact binary input (e.g., 4 bits) and activates one of many output lines (e.g., to select one of 16 registers). **Trade-off:** Saves control store bits but adds delay to the critical path, potentially forcing a longer clock cycle. |
| **Critical Path**                       | The sequence of operations within a single clock cycle that takes the longest time to complete. This path determines the minimum possible cycle time.                                                                                                           |
| **Technique 1: Merge Interpreter Loop** | Embedding the main instruction fetch/decode cycle into the end of each instruction's microcode sequence. This eliminates a dedicated cycle for this overhead.                                                                                                   |
| **Technique 2: Three-Bus Architecture** | Adding a second input bus (A bus) to the ALU alongside the original B bus. This allows two registers to be added directly in one cycle without first copying one to a temporary register (H).                                                                   |
| **Two-Bus Design Limitation**           | Requires a temporary register (H) to hold one operand for ALU operations, often adding an extra cycle.                                                                                                                                                          |
| **Instruction Fetch Unit (IFU)**        | A specialized, semi-autonomous hardware unit that pre-fetches instructions and assembles multi-byte operands, freeing the main ALU from these tasks.                                                                                                            |
| **Technique 3: Use an IFU**             | Implements an IFU (Instruction Fetch Unit) to handle PC incrementing and instruction/operand fetching in parallel with main execution, dramatically reducing path length.                                                                                       |
| **MBR1 / MBR2**                         | Registers in the IFU. MBR1 holds the next single byte; MBR2 holds the next two bytes (16-bit value). They provide sign-extended and zero-extended versions to the B bus.                                                                                        |
| **Finite State Machine (FSM)**          | A model used to design the IFU's control logic, consisting of states (e.g., bytes in buffer) and transitions (e.g., reading a byte, fetching a word).                                                                                                           |
| **Pipelining**                          | A design technique that overlaps the execution of multiple instructions by dividing the data path into distinct stages, each working on a different instruction simultaneously.                                                                                 |
| **Pipeline Stage**                      | One segment of a pipelined processor. In the Mic-3, the stages are: 1. IFU, 2. A&B Latch Load, 3. ALU/Shifter, 4. Register Write-back.                                                                                                                          |
| **Throughput**                          | The number of instructions completed per unit of time. Pipelining aims to maximize throughput, achieving one instruction completion per cycle in the ideal case.                                                                                                |
| **Latency**                             | The time taken to complete a single instruction from start to finish. Pipelining does not reduce latency (it may increase it slightly) but improves throughput.                                                                                                 |
| **Latch (A, B, C)**                     | Temporary registers inserted into the data path (e.g., in the Mic-3) to break it into stages. They hold intermediate values between pipeline stages.                                                                                                            |
| **True Dependence (RAW Hazard)**        | A situation in a pipeline where an instruction requires the result of a previous instruction that has not yet been produced. "Read After Write."                                                                                                                |
| **Hazard**                              | Any situation in a pipeline that prevents an instruction from executing in its designated clock cycle.                                                                                                                                                          |
| **Stalling**                            | The action of halting the pipeline for one or more cycles to resolve a hazard, such as waiting for a result from a previous instruction.                                                                                                                        |
| **Mic-1**                               | The baseline, simple microarchitecture with a two-bus data path and no pipelining.                                                                                                                                                                              |
| **Mic-2**                               | An enhanced microarchitecture that adds an **Instruction Fetch Unit (IFU)** and merges the interpreter loop to reduce path length.                                                                                                                              |
| **Mic-3**                               | A **pipelined** microarchitecture based on the Mic-2. It uses latches to break the data path into stages, allowing a higher clock speed and the overlapped execution of instructions.                                                                           |
| **Assembly Line Analogy**               | A classic analogy for pipelining: just as a car factory produces one finished car per minute despite the total assembly time being much longer, a pipeline aims to complete one instruction per cycle.                                                          |
### Optimization / improving performance

|Term|Definition|
|---|---|
|**Implementation Improvements**|Changes to how a CPU is built (e.g., faster clock, better circuits) that make it run faster **without changing the Instruction Set Architecture (ISA)**. This ensures backward compatibility.|
|**Architectural Improvements**|Changes that modify the ISA itself (e.g., adding new instructions or registers). Old programs may still run, but require recompilation to take full advantage of new features.|
|**Memory Latency**|The delay between the CPU requesting data from memory and the data being delivered. A primary bottleneck for performance.|
|**Memory Bandwidth**|The amount of data that can be transferred from memory to the CPU per unit of time.|
|**Split Cache (Harvard Architecture)**|A design using separate caches for instructions (I-cache) and data (D-cache). This allows both an instruction and a data operand to be fetched simultaneously, doubling bandwidth.|
|**Multilevel Cache Hierarchy**|A system with multiple levels of cache (L1, L2, L3). L1 is smallest and fastest (on the CPU chip), L2 is larger and slower, and L3 is even larger and slower, acting as a buffer to main memory (DRAM).|
|**Inclusive Cache**|A property where the contents of a smaller, faster cache (e.g., L1) are also present in a larger, slower cache (e.g., L2). Simplifies coherence but uses more cache space.|
|**Spatial Locality**|The tendency that if a memory location is accessed, nearby memory locations are likely to be accessed soon. Caches exploit this by fetching data in blocks ("cache lines").|
|**Temporal Locality**|The tendency that if a memory location is accessed, it is likely to be accessed again soon. Caches exploit this by keeping recently used data in the fast cache.|
|**Cache Line (Block)**|The fixed-size unit of data transferred between main memory and the cache. A single memory access fetches an entire line (e.g., 32 or 64 bytes), not just the requested word.|
|**Direct-Mapped Cache**|The simplest cache organization where each memory block can be placed in exactly **one** location in the cache. Determined by the `LINE` bits of the memory address.|
|**Cache Entry Components**|1. **Valid Bit:** Indicates if the entry contains valid data.  <br>2. **Tag:** A unique identifier for the memory block stored in this cache entry.  <br>3. **Data:** The actual cached data (a full cache line).|
|**Cache Hit**|The situation where the data requested by the CPU is found in the cache. This results in very fast access.|
|**Cache Miss**|The situation where the data requested by the CPU is **not** in the cache. This forces a slow access to main memory to fetch the entire cache line.|
|**Collision (in Direct-Mapped Cache)**|A problem where two frequently used memory blocks map to the same cache line, causing them to constantly evict each other, leading to poor performance.|
|**Set-Associative Cache**|A cache organization that helps reduce collisions. Each memory block can be placed in **`n`** possible locations in the cache (a "set").|
|**n-way Set-Associative**|A cache where each set contains `n` cache entries. Common configurations are 2-way, 4-way, or 8-way.|
|**LRU (Least Recently Used)**|A common cache replacement algorithm. When a new line must be brought into a full set, the cache entry that was used least recently is evicted. It exploits temporal locality.|
|**Fully-Associative Cache**|A cache where a memory block can be placed in **any** location. This eliminates collisions but is impractical for large caches due to the high cost of searching all entries in parallel.|
|**Write Through**|A cache write policy where data is written to **both the cache and main memory** simultaneously. Simpler and more reliable, but generates more memory traffic.|
|**Write Back (Write Deferred)**|A cache write policy where data is written only to the cache. The main memory is updated later, only when the cache line is evicted. Reduces memory traffic but is more complex.|
|**Write Allocation**|A policy on a write miss: the cache fetches the entire cache line containing the write address into the cache and then performs the update. Often used with **write back** policies.|
|**No-Write Allocation**|A policy on a write miss: the data is written directly to main memory without being brought into the cache. Often used with **write through** policies for simplicity.|