![[Pasted image 20251009190624.png]]
### Pipelining

| Term                                 | Definition                                                                                                                                                                                                                                                                 |
| ------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Pipelining**                       | A processing technique where the execution of an instruction is divided into multiple discrete stages. Each stage is handled by dedicated hardware, allowing multiple instructions to be processed simultaneously at different stages.                                     |
| **Instruction Fetch (Stage 1)**      | The pipeline stage that retrieves the next instruction from memory.                                                                                                                                                                                                        |
| **Instruction Decode (Stage 2)**     | The pipeline stage that deciphers the instruction to determine its type and identify the operands it requires.                                                                                                                                                             |
| **Operand Fetch (Stage 3)**          | The pipeline stage that locates and retrieves the required operands, either from registers or from memory.                                                                                                                                                                 |
| **Instruction Execution (Stage 4)**  | The pipeline stage that performs the actual operation of the instruction, typically by processing the operands through the CPU's data path (e.g., the ALU).                                                                                                                |
| **Write Back (Stage 5)**             | The pipeline stage that writes the result of the instruction execution back to the appropriate register.                                                                                                                                                                   |
| **Pipeline Stage / Unit**            | A dedicated section of hardware within the pipeline responsible for one specific phase of instruction processing.                                                                                                                                                          |
| **Latency**                          | The total time required to complete a single instruction from start to finish. In an n-stage pipeline with a cycle time T, the latency is **n * T**.                                                                                                                       |
| **Processor Bandwidth (Throughput)** | The number of instructions the CPU can complete per second. It is a measure of the pipeline's processing rate.                                                                                                                                                             |
| **Throughput Calculation**           | With a cycle time of T nsec, the processor can complete **1/T** billion instructions per second. This translates to **1000/T MIPS** (Millions of Instructions Per Second).                                                                                                 |
| **Prefetching**                      | An early technique to speed up execution by fetching instructions from memory in advance and storing them in a buffer (prefetch buffer) so they are ready when needed. Pipelining is a more advanced form of this concept.                                                 |
| **Prefetch Buffer**                  | A special set of registers that holds instructions fetched from memory in advance of their execution.                                                                                                                                                                      |
| **Trade-off**                        | Pipelining creates a trade-off: it increases **processor bandwidth (throughput)** by allowing a new instruction to complete on every clock cycle, but it does not reduce the **latency** of a single instruction (it may even increase it slightly due to stage overhead). |
| **Analogy**                          | Like an assembly line in a cake factory, where different workers (stages) perform specific tasks (placing box, adding cake, sealing, labeling) on different cakes (instructions) simultaneously.                                                                           |
### Cache

| Term                                   | Definition                                                                                                                                                                                         |
| -------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Cache Memory**                       | A small, very fast piece of memory located between the CPU and main memory. It stores the most heavily used memory words to dramatically reduce average memory access time.                        |
| **Memory-CPU Speed Gap**               | The fundamental problem where CPUs are significantly faster than main memory (DRAM). After issuing a memory request, the CPU must wait many cycles for the data to arrive, stalling execution.     |
| **Stalling**                           | A simple solution to the memory gap: the CPU halts (stalls) execution when an instruction tries to use a memory word that has not yet arrived from slow memory. This severely impacts performance. |
| **Locality Principle**                 | The observation that memory references in any short time interval tend to cluster in a few areas of memory. This principle is the foundation that makes caching effective.                         |
| **Temporal Locality**                  | The tendency for a memory location, once accessed, to be accessed again soon. (e.g., variables in a loop).                                                                                         |
| **Spatial Locality**                   | The tendency for a memory location, once accessed, to have its nearby locations accessed soon. (e.g., sequential instruction execution, array traversal).                                          |
| **Hit Ratio (h)**                      | The fraction of all memory references that are satisfied by the cache. A high hit ratio is crucial for good performance.                                                                           |
| **Miss Ratio**                         | The fraction of memory references that are _not_ in the cache. Calculated as `1 - h`.                                                                                                              |
| **Cache Hit**                          | When the CPU finds the data it needs in the cache. This results in a fast access.                                                                                                                  |
| **Cache Miss**                         | When the data the CPU needs is _not_ in the cache. This forces a slow access to main memory.                                                                                                       |
| **Mean Access Time**                   | The average time to access memory. Formula: **`c + (1 - h)m`**, where `c` is cache access time, `m` is main memory access time, and `h` is the hit ratio.                                          |
| **Cache Line (Block)**                 | The fixed-size unit of data that is transferred between main memory and the cache. On a miss, the entire line containing the requested word is fetched.                                            |
| **Line Size (Block Size)**             | The number of bytes in a cache line. Larger lines exploit spatial locality more effectively.                                                                                                       |
| **Unified Cache**                      | A single cache that holds both instructions and data. Simpler design but cannot service an instruction fetch and a data fetch simultaneously.                                                      |
| **Split Cache (Harvard Architecture)** | Separate caches for instructions (I-cache) and data (D-cache). Allows a pipelined processor to fetch an instruction and a data operand in parallel.                                                |
| **Primary Cache (L1)**                 | The smallest and fastest cache, typically located directly on the CPU chip.                                                                                                                        |
| **Secondary Cache (L2)**               | A larger, slower cache that sits between the L1 cache and main memory. It may be on-chip or in the same package as the CPU.                                                                        |
| **Tertiary Cache (L3)**                | An even larger, slower cache that is shared between multiple CPU cores, sitting further from the core than L2.                                                                                     |
| **Cache Design Trade-offs**            | The balancing act between cache **size** (bigger is better for hit ratio but slower to access), **line size**, **organization**, and **number of levels**.                                         |
![[Pasted image 20251009194906.png]]